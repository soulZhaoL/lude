# 排除因子配置优化 - 对话记录与改进计划

## 对话背景

**时间**: 2025-08-01  
**问题核心**: 排除因子配置的合理性问题，主要涉及 `filter_factors_config.yaml` 的参数设置策略

## 关键问题识别

### 1. 用户回退了之前的配置变更

用户认为之前的变更方向不对，主要原因：

- value_options 过于密集的担忧可能是误判
- 设置过多排除因子导致过度过滤，是过拟合的主要原因
- 类似 `theory_conv_prem >= 1.0` 的条件会排除掉绝大多数标的

### 2. 用户的核心洞察

1. **单条件+密集选项的合理性**: 如果 `max_conditions: 1`，密集的 value_options 反而能让优化算法找到更精准的阈值
2. **过度过滤的致命问题**: 头部/尾部的极端阈值会把99%的标的排除，导致无候选标的
3. **异常检测思路**: 排除因子应该用于数据清洗，类似异常检测，而不是策略选择

## 分析师反馈要点

### ✅ 用户说对的地方

1. **单条件策略正确**: `max_conditions: 1` + 密集 value_options 确实比粗糙分割更优
2. **过度过滤识别准确**: `theory_conv_prem >= 1.0` 等条件确实会过度排除标的
3. **异常检测方向正确**: 排除因子主要用于数据清洗而非策略选择

### 🚨 指出的盲点和问题

#### 1. 数据分布认知不足

- 用户提到"异常检测"但缺乏对各因子实际分布的了解
- 需要了解每个因子的正常范围、均值、标准差、分位数等基础统计特征
- 缺乏基于3σ原则或IQR方法的异常值边界设定

#### 2. 业务逻辑验证缺失

异常检测不能纯统计，需要结合业务逻辑：

- `left_years < 0.1`: 统计正常但业务无意义
- `amount < 500`: 异常流动性风险信号
- `conv_prem > 2.0`: 统计存在但投资无价值

## 当前配置状态

当前 `filter_factors_config.yaml` 中激活的排除因子：

- `turnover`: 换手率 (10个选项: 0.001~1.5)
- `conv_prem`: 转股溢价率 (6个选项: -0.4~0.6)
- `pct_chg`: 涨跌幅 (6个选项: -0.6~1.0)
- `pct_chg_5`: 5日涨跌幅 (5个选项: -0.3~1.0)
- `pct_chg_5_stk`: 正股5日涨跌幅 (5个选项: -0.3~1.0)
- `bias_5`: 5日乖离率 (10个选项: -0.01~0.01)
- `close_ma_5`: 5日均价 (5个选项: 100~190)
- `alpha_pct_chg_5`: 5日超额涨跌幅 (6个选项: -0.02~0.03)
- `turnover_5`: 5日换手率 (9个选项: 0.03~0.5)
- `theory_conv_prem`: 理论溢价率 (9个选项: -0.3~1.0)
- `mod_conv_prem`: 修正溢价率 (8个选项: -0.2~0.8)
- `bond_prem`: 纯债溢价率 (8个选项: -0.4~2.0)
- `ytm`: 到期收益率 (8个选项: -0.05~0.3)
- `theory_bias`: 理论偏离度 (7个选项: -0.2~0.5)
- `pure_value`: 纯债价值 (7个选项: 70~150)
- `theory_value`: 理论价值 (5个选项: 90~150)
- `conv_value`: 转股价值 (5个选项: 80~160)
- `option_value`: 期权价值 (5个选项: 2~80)
- `issue_size`: 发行规模 (5个选项: 10~200)

总计：19个激活因子，combination_rules 限制最多3个因子组合。

## 改进计划

### 🎯 第一阶段：数据探索与分布分析 (优先级：高)

#### 1.1 基础统计分析

```bash
# 执行数据分布分析
source ~/miniconda3/etc/profile.d/conda.sh && conda activate lude && python -c "
import pandas as pd
import numpy as np
from lude.config.paths import PARQUET_DATA_PATH

# 读取数据
df = pd.read_parquet(PARQUET_DATA_PATH)

# 当前激活的排除因子列表
active_factors = [
    'turnover', 'conv_prem', 'pct_chg', 'pct_chg_5', 'pct_chg_5_stk',
    'bias_5', 'close_ma_5', 'alpha_pct_chg_5', 'turnover_5',
    'theory_conv_prem', 'mod_conv_prem', 'bond_prem', 'ytm',
    'theory_bias', 'pure_value', 'theory_value', 'conv_value',
    'option_value', 'issue_size'
]

# 统计分析
print('=== 排除因子数据分布分析 ===')
for factor in active_factors:
    if factor in df.columns:
        series = df[factor].dropna()
        print(f'\n{factor}:')
        print(f'  样本数: {len(series)}')
        print(f'  均值: {series.mean():.4f}, 标准差: {series.std():.4f}')
        print(f'  最小值: {series.min():.4f}, 最大值: {series.max():.4f}')
        print(f'  分位数: 1%={series.quantile(0.01):.4f}, 5%={series.quantile(0.05):.4f}, 95%={series.quantile(0.95):.4f}, 99%={series.quantile(0.99):.4f}')
        
        # 异常值检测 (3σ原则)
        mean_val = series.mean()
        std_val = series.std()
        outlier_lower = mean_val - 3 * std_val
        outlier_upper = mean_val + 3 * std_val
        outlier_count = len(series[(series < outlier_lower) | (series > outlier_upper)])
        print(f'  3σ异常值边界: [{outlier_lower:.4f}, {outlier_upper:.4f}], 异常值数量: {outlier_count} ({outlier_count/len(series)*100:.2f}%)')
    else:
        print(f'{factor}: 列不存在')
"
```

#### 1.2 业务逻辑异常值定义

基于可转债投资逻辑，定义各因子的业务异常边界：

**价格类因子**:

- `close_ma_5`: < 70 或 > 300 (价格异常)
- `conv_value`, `theory_value`, `pure_value`: < 50 或 > 500 (价值异常)

**溢价率类因子**:

- `theory_conv_prem`, `mod_conv_prem`: < -0.5 或 > 3.0 (溢价异常)
- `bond_prem`: < -0.8 或 > 5.0 (纯债溢价异常)

**流动性类因子**:

- `turnover`, `turnover_5`: < 0.0001 或 > 5.0 (换手率异常)
- `issue_size`: < 5 或 > 1000 (规模异常)

**收益率类因子**:

- `ytm`: < -0.2 或 > 1.0 (收益率异常)
- `pct_chg`, `pct_chg_5`, `pct_chg_5_stk`: < -0.8 或 > 3.0 (涨跌幅异常)

### 🎯 第二阶段：配置优化与验证 (优先级：高)

#### 2.1 基于分布的异常检测配置

根据第一阶段的分析结果，重新设计 value_options：

- 使用 [1%分位数, 5%分位数, 95%分位数, 99%分位数] 作为异常检测边界
- 结合业务逻辑边界进行修正
- 确保每个阈值都有实际的过滤意义

#### 2.2 过滤效果验证

```bash
# 验证各阈值的过滤效果
source ~/miniconda3/etc/profile.d/conda.sh && conda activate lude && python -c "
# 测试各个阈值的数据保留比例
# 确保没有阈值会过度过滤（如保留率 < 5%）
"
```

#### 2.3 因子重要性评估

- 分析哪些排除因子对最终性能影响最大
- 考虑减少低价值因子，降低组合复杂度
- 重点保留高价值的数据清洗因子

### 🎯 第三阶段：系统性测试与优化 (优先级：中)

#### 3.1 A/B测试框架

- 对比当前配置 vs 优化后配置的性能表现
- 使用相同的优化参数进行公平比较
- 关注CAGR、夏普比率、最大回撤等关键指标

#### 3.2 过拟合风险评估

- 在不同时间窗口测试配置稳定性
- 分析排除因子数量与性能的关系
- 建立过拟合预警机制

### 🎯 第四阶段：文档与流程标准化 (优先级：低)

#### 4.1 排除因子设计原则文档化

- 异常检测 vs 策略选择的边界
- 各类因子的业务逻辑约束
- 阈值设定的统计学原则

#### 4.2 配置管理流程

- 建立配置变更的验证流程
- 自动化的配置合理性检查
- 版本化管理与回滚机制

## 执行计划时间线

### 立即执行 (下次对话重点)

1. **数据分布分析** - 运行基础统计分析脚本，了解各因子真实分布
2. **异常边界识别** - 基于分位数和业务逻辑确定合理的异常检测阈值
3. **过滤效果预估** - 评估新阈值的数据保留率，避免过度过滤

### 短期执行 (1-2天内)

1. **配置文件重构** - 基于分析结果更新 `filter_factors_config.yaml`
2. **验证测试** - 小规模测试新配置的优化效果
3. **性能对比** - 与当前配置进行A/B对比

### 中期执行 (1周内)

1. **全面测试** - 在完整数据集上测试新配置
2. **稳定性验证** - 多时间窗口的鲁棒性测试
3. **文档更新** - 更新配置文件注释和设计文档

## 关键风险提醒

### ⚠️ 数据质量风险

- 确保统计分析使用的是完整、准确的数据集
- 注意处理缺失值和异常值对统计结果的影响

### ⚠️ 过度优化风险

- 避免基于单一时间段的数据过度调优
- 保持异常检测的通用性，不要过于针对特定市场环境

### ⚠️ 业务逻辑风险

- 统计异常不等于业务异常，需要领域专家验证
- 某些"异常"可能是重要的投资机会信号

## 成功标准

### 定量指标

1. **数据保留率**: 每个阈值的数据保留率在10%-90%之间
2. **CAGR提升**: 相比当前配置，CAGR提升至少5%
3. **稳定性**: 在不同时间窗口的性能波动 < 20%

### 定性指标

1. **逻辑合理性**: 所有阈值都有明确的业务逻辑支撑
2. **可解释性**: 配置参数能够向业务人员清晰解释
3. **维护性**: 配置更新流程标准化，便于后续调整

---

**下次对话重点**: 首先执行数据分布分析，基于真实数据制定异常检测策略！